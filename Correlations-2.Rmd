---
title: "Correlations"
author: "Josh Jackson"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  ioslides_presentation: default
---
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
## class 2
>>>>>>> 16b537c3919937dca06628fbedd8f713488e8a0b
>>>>>>> c5599fb69d535e9b45596e4d7afdf6be15cd5fea

## Relationships

- What is the relationship between IV and DV?

  * Measuring relationships depend on type of measurement
  * E.g. chi-square for categorical
  
## Modeling relationships

```{r, eval=FALSE}
t.test(y ~ x, data)
```
- This semester we will examine this relationship through GLM that takes the form for y = x + e
- What analysis you use will depend on how you measure your IV (X) and your DV (Y)

## Modeling relationships

- You have primarily been working wtih categorical IVs (t-test, ANOVA, chi-square)
- How do we look at continous IVs and DVs? 

## Dispersion

Variation (sum of squares)
$$ SS = {\sum{(x-\bar{x})^2}} $$
$$ SS = {\sum{(x-\mu)^2}} $$

## Dispersion
Variance
$$ s^{2} = {\frac{\sum{(x-\bar{x})^2}}{N-1}} $$
$$ \sigma^{2} = {\frac{\sum{(x-\mu)^2}}{N}} $$

## Dispersion
Standard Deviation
$$ s = \sqrt{\frac{\sum{(x-\bar{x})^2}}{N-1}} $$
$$ \sigma = \sqrt{\frac{\sum{(x-\mu)^2}}{N}} $$

## formula standard error of the mean? 

## Associations
Covariation (cross products)
$$ SS = {\sum{{(x-\bar{x})(y-\bar{y})}}} $$
$$ SS = {\sum{{(x-\mu_{x})(y-\mu_{y})}}} $$

## Associations
Covariance
$$ cov_{xy}^{2} = {\frac{\sum{(x-\bar{x})(y-\bar{y})}}{N-1}} $$
$$ \sigma_{xy}^{2} = {\frac{\sum{(x-\mu_{x})(y-\mu_{y})}}{N}} $$

>- Covariance matrix is basis for many analyses
>- What are some issues that may arise when comparing covariances? 

## Associations
Correlations
$$ r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}} $$

$$ \rho_{xy} = {\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}} $$


Many other formulas exist for specific types of data, these were more helpful when we computed everything by hand (more on this later)


## Z-scores
$$ r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}} $$

What are the characteristics of z-scores? 
$$ z = {\frac{\sum{(x-\bar{x})}}{N}} $$



## Associations
Correlations

- How much two variables are linearly related
- -1 to 1 
- Invariant to changes in mean or scaling
- Most common (and basic) effect size measure

```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psych)
library(ggplot2)
galton.data <- galton
```

## Associations
Correlations
```{r, echo=FALSE, cache=TRUE}
ggplot(galton.data, aes(x=parent, y=child)) +
    geom_point() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```



## Correlations 
###Hypothesis testing

$$ H_{0}: \rho_{xy} = 0 $$
$$ H_{A}: \rho_{xy} \neq 0 $$

>- Assumes: 
>- Observations are independent
>- Symmetric bivariate distribution (joint probability distribution)


----
```{r, echo=FALSE,cache=TRUE}
mu1<-0 # setting the expected value of x1
mu2<-0 # setting the expected value of x2
s11<-10 # setting the variance of x1
s12<-15 # setting the covariance between x1 and x2 
s22<-10 # setting the variance of x2
rho<-0.5 # setting the correlation coefficient between x1 and x2 
x1<-seq(-10,10,length=41) # generating the vector series x1 
x2<-x1 # copying x1 to x2
f<-function(x1,x2)
{
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2))) 
term2<--1/(2*(1-rho^2))
term3<-(x1-mu1)^2/s11
term4<-(x2-mu2)^2/s22 
term5<--2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
} # setting up the function of the multivariate normal density
#
z<-outer(x1,x2,f) # calculating the density values
#
persp(x1, x2, z,
main="Joint Probability Distribution", sub=expression(italic(f)~(bold(x))==frac(1,2~pi~sqrt(sigma[11]~ sigma[22]~(1-rho^2)))~phantom(0)^bold(.)~exp~bgroup("{", list(-frac(1,2(1-rho^2)),
bgroup("[", frac((x[1]~-~mu[1])^2, sigma[11])~-~2~rho~frac(x[1]~-~mu[1], sqrt(sigma[11]))~ frac(x[2]~-~mu[2],sqrt(sigma[22]))~+~ frac((x[2]~-~mu[2])^2, sigma[22]),"]")),"}")),
col="lightgreen",
theta=30, phi=20,
r=50,
d=0.1,
expand=0.5,
ltheta=90, lphi=180,
shade=0.75,
ticktype="detailed",
nticks=5) 

# produces the 3-D plot
mtext(expression(list(mu[1]==0,mu[2]==0,sigma[11]==10,sigma[22]==10,sigma[12 ]==15,rho==0.5)), side=3) 
# adding a text line to the graph


```


## Correlations 
###Hypothesis testing

$$ H_{0}: \rho_{xy} = 0 $$
$$ H_{A}: \rho_{xy} \neq 0 $$

Test statistic

$$  t = {\frac{r}{SE_{r}}} $$
$$  t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}} $$

df = N-2

## Effect size 

>- The strength of relationship between two variables

>- Ω2, η2, cohen’s d, cohen’s f, hedges g, R2 , Risk-ratio, etc

>- Significance is a function of effect size and sample size

>- Statistical significance ≠ practical significance

## Effect size  
###How big is practical?

>- Cohen (.1, .3., .5)
>- Meyer & Hemphill .3 is average
>- Rosenthaul
>-
Drug TX?  | Alive |  Dead
----------|-------|--------
Treatment |  65   |  35
No Tx     |  35   |  65

>- Variance explained (more later)

##What is the size of the correlation?
- Chemotherapy and breast cancer survival?   
- Batting ability and hit success on a single at bat?   
- Antihistamine use and reduced sneezing/runny nose?   
- Combat exposure and PTSD?   
- Ibuprofen on pain reduction?   
- Gender and weight?   
- Therapy and well being?   
- Observer ratings of attractiveness?   
- Gender and arm strength?   
- Nearness to equator and daily temperature for U.S.?   

##What is the size of the correlation?
- Chemotherapy and breast cancer survival? (.03)   
- Batting ability and hit success on a single at bat? (.06)   
- Antihistamine use and reduced sneezing/runny nose? (.11)   
- Combat exposure and PTSD? (.11)   
- Ibuprofen on pain reduction? (.14)   
- Gender and weight? (.26)   
- Therapy and well being? (.32)   
- Observer ratings of attractiveness? (.39)   
- Gender and arm strength? (.55)   
- Nearness to equator and daily temperature for U.S.? (.60)

##Questions to ask yourself:
What is your N?  
What is the typical effect size in the field?  
Study design?  
What is your DV?  
Importance (reaction time vs cancer)?  
Same method as IV (method variance)?  

##Power calculations
```{r}
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```

##Power calculations
- But what is your confidence? 
- N = 84 gives you CI[.09, 48]
- Schönbrodt & Perugini (2013) suggest correlations stabilize at 250+ regardless of effect
- CI[.19, .39]

## Fisher’s r to z’ transformation
- If we want to make calculations based on $\rho \neq 0$ then we will run into a skewed sampling distribution
 
```{r,echo=FALSE, cache=TRUE }
library(ggplot2)

r=0.95
n=50
rep=10000
z=.5*log((1+r)/(1-r))
se=1/sqrt(n-3)
zvec=rnorm(rep)*se+z
ivec=(exp(2*zvec)-1)/(exp(2*zvec)+1)
correlation <- as.data.frame(ivec)
correlation$cor <- correlation$ivec
ggplot(correlation, aes(cor)) + geom_histogram(breaks=seq(.85, .99, by = .001), aes(y = ..density..)) + geom_density()
```

## Fisher’s r to z’ transformation
- Skewed sampling distribution will rear its head when: 
    * $H_{0}: \rho \neq 0$
    * Calculating confidence intervals
    * Testing two correlations against one another
- r to z': 

$$ z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}} $$

## Fisher’s r to z’ transformation

<img src="/Users/Jackson/Box Sync/5067 Regression Spring/regression/fisher.png" width="600" height="500"/>

##
Steps for computing confidence interval
1. Transform r into z'
2. Compute CI as you normally would using z'
3. revert back to r

$$ r = {\frac{e^{2z'}-1}{e^{2z'}+1}} $$

## How to do in R
```{r, eval=FALSE}
library(psych)
fisherz(r)
fisherz2r(z)
```

## Two independent groups test
-Does the correlation in group 1 differ from the correlation in group 2? 
$$ H_{A}: \rho_{1} = \rho_{2} $$
$$ H_{A}: \rho_{1} \neq \rho_{2} $$
-Normally distributed
$$ Z = {\frac{z'_{1}-z'_{2}}{se_{z1-z2}}} $$

## Two independent groups test
- different standard error compared to 1 sample test
$$ Z = {\frac{z'_{1}-z'_{2}}{se_{z1-z2}}} $$
$$ se_{z1-z2} = {\sqrt {se_{z1} + se_{z1}}} = {\sqrt {\frac{1}{n_{1}-3}+{\frac{1}{n_{2}-3}}}} $$


## Other correlation tests:
1. Set of correltions
2. Dependent correlations (i.e., within same group)  
  These are more easily tested via Structural Equation Modeling (SEM)
3. Intra Class Correlation (ICC)
  
## Factors that influence r (and most other test statistics)
1. Restriction of range (GRE scores and success)
2. Very skewed distributions (smoking and health)
3. Non-linear associations
4. Measurement overlap (modality and content)
5. Reliability

## Reliability
- All measurement includes error
- Score = true score + measurement error (CTT version)
- Reliability assesses the consistency of measurement
  
## Reliability
- All measurement includes error
- Score = true score + measurement error
- Reliability assesses the consistency of measurement
  
  Which would you rather have? 
  - 1 item final exam versus 30 item? 
  - assessment via trained clinician vs tarot cards ?
  - fMRI during minor earthquake vs no earthquake?
  
## Reliability

- Cannot correlate error (randomness) with something
- Because we do not measure our variables perfectly we get lower correlations compared to true correlations
- If we want to have a valid measure it better be a reliable measure

## Reliability
- think of reliability as a correlation with a measure and itself in a different world, at a different time, or a different but equal version

$$ r_{XX} $$

## Reliability
- true score variance divided by observed variance
- how do you assess theoretical variance i.e., true score variance? 

$$ r_{XY} = r_{X_{T} Y_{T}} {\sqrt {r_{XX} r_{YY}}} $$
$$ r_{XY} = .6 {\sqrt {(.70) (.70)}} $$
## Reliability

$$ r_{X_{T} Y_{T}} =  = {\frac {r_{XY}} {\sqrt{{r_{XX} r_{YY}}}}} $$

$$ r_{X_{T} Y_{T}} =  = {\frac {.30} {\sqrt{(.70)(.70)}}} $$
## Reliability

- if you are going to measure something do it well
- applies to ALL IVs and DVs, and all designs
- remember this when interpretting other's research

##Types of correlations


##Types of correlations
1. Point Biserial
    +  continuous and dichtomous
2. Phi coefficient
    + both dichotomous
3. Spearman rank order
    + ranked data (nonparametric)
4. Biserial (assumes dichotomous is continous)
5. Tetrachoric (assumes dichotomous is continous)
    + both dichotomous
6. Polychoric (assumes continous)
    + ordinal
  



