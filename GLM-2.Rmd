---
title: "GLM"
output: ioslides_presentation
widescreen: true
---

## Models

- Thus far you have used t-tests and ANOVA to test basic questions about the world. 
- These types of tests can be thought of as a model for how you think "the world works"
- Our DV (hereforth Y) is what we are trying to understand
- We hypothesize it has some relationship with your IV(s) (hereforth Xs)  

- Y = X + E

## See this in our R code

Independent samples t-test
```{r, eval=FALSE}

t.1 <- t.test(y ~ x, data = d) 
# y is cont and x is a categoriocal/nominal (dichotomous) factor
```

One-way ANOVA
```{r, eval = FALSE}
a.1 <- aov(y ~ x, data=d)
# y is cont and x is a categoriocal/nominal factor

```

## General linear model

- This model (equation) can be very simple as in a treatment/control experiment  
- It can be very complex in terms of trying to understand something like academic achievment  
- The majority of our models fall under the umbrella of a general(ized) linear model
- All models imply our theory about how the data are generated (ie how the world works)


## Regression Equation

$$ Y_i = b_{0} + b_{1}X_i +e_i  $$




## Regression terms
- Y/DV/Outcome/Response/Criterion
- X/IV/Predictor/Explanatory variable
- Regression coeffiecent (weight)/b/b*/$\beta$
- Intercept bo/ $\beta_{0}$
- Error/Residuals $e$
- Predictions  $\hat{Y}$

## Regression models

- These models are a way to convey the relationship between two (or more) variables
- We can use these to get information we may be interested in (e.g. means)


## Example
```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(readr)
example.data <- read_csv("example.data.csv")

```

- Get this data at my github account: https://github.com/josh-jackson/Psych5067

```{r}
example.data
```

##

```{r, warning=FALSE}
ggplot(example.data) +
  aes(x = as.factor(tx), y = traffic.risk) +
  geom_violin() + geom_boxplot() + geom_jitter()
```

## example
```{r}
t.1 <- t.test(traffic.risk ~ tx, data = example.data, var.equal = TRUE) 
t.1

```

## example
```{r}
a.1 <- aov(traffic.risk ~ tx, data = example.data) 
summary(a.1)
```

## example

```{r, eval = FALSE}
mod.1 <- lm(traffic.risk ~ tx, data = example.data)
summary(mod.1)
```

## example

```{r, echo = FALSE}
mod.1 <- lm(traffic.risk ~ tx, data = example.data)
summary(mod.1)
```


## example cont
```{r}
mod.1 <- lm(traffic.risk ~ tx, data = example.data)
anova(mod.1)
```

## example summary
Same p-values for each test; same SS; same test!   
<br>    

- t-test gives you a t & df (output may give you group mean and SD)    
- ANOVA gives you an F (and SSs)  
- linear model (regression) gives you an equation  


## ANOVA as regression

$$ Y_i = b_{0} + b_{1}X_i + e_i  $$
$$ T.risk_i = b_{0} + b_{1}TX_i + e_i  $$

- Each individual has a unique Y value an X value and a residual/error term  

- The model only has a single $b_{0}$ and $b_{1}$ term. These are the regression parameters. $b_{0}$ is the intercept and $b_{1}$ quantifies the relationship between your model of the world and the DV. 

## What do the estimates tell us? 

```{r, echo = FALSE}
mod.1 <- lm(traffic.risk ~ tx, data = example.data)
library(broom)
tidy(mod.1)
```


## 

```{r}
example.data %>% 
  group_by(tx) %>% 
  summarise(mean(traffic.risk, na.rm=TRUE))
```

## How to interpret regression estimates

- Intercept is the mean of group of variable tx that is coded 0
- Regression coefficient is the difference in means between the groups (ie slope)

##
```{r, warning=FALSE, echo = FALSE}
library(dplyr)
ggplot(example.data %>% filter(!is.na(tx))) +
  aes(x = as.factor(tx), y = traffic.risk) +
  geom_violin() + geom_boxplot() + geom_jitter()
```



##
```{r, echo=FALSE, cache=TRUE, warning= FALSE}
library(ggplot2)
ggplot(example.data, aes(x=tx, y=traffic.risk)) +
    geom_point() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE)    # Don't add shaded confidence region
    
```

## How to interpret regression estimates
- The entire class will go over different ways to interpret these estimates/parameters/coefficients

- Intercept (B0) signifies the level of Y when your model IVs (Xs) are zero 
- Regression (B1) signifies the difference for a one unit change in your X

- as with last semester you have estimates (like $\bar{x}$) and standard errors, which you can then ask whether they are likely assuming a null or create a CI

## predicted scores

- Based on the output how do I calculate means for each group? 
```{r}
tidy(mod.1)
```


## ANOVA as regression
- Nominal/categorical variables do not have any inherant numbers associated with them
- Need to assign them numbers
- What numbers you assign will impact the equation/estimates/hypothesis you can test  

## ANOVA as regression
$$ T.risk_i = b_{0} + b_{1}TX_i + e_i  $$

```{r, echo =FALSE}
example.data
```

## 
```{r, echo = FALSE}
library(dplyr)
example.data$tx.r <- as.factor(example.data$tx)
example.data$tx.r <- recode_factor(example.data$tx.r, "0" = "control", "1" = "treatment") 
```

What happens if we didn't have numbers in our dataframe? 
```{r}
example.data
```


##

```{r}
mod.1 <- lm(traffic.risk ~ tx, data = example.data)
tidy(mod.1)
```

```{r}
mod.1r <- lm(traffic.risk ~ tx.r, data = example.data)
tidy(mod.1r)
```


## what about if you had different values other than 0 and 1? 

- Infinite number of ways to code categorical variables, only a few meaningful ways  
- The R default is called "dummy coding"  
- Uses 0s and 1s to put numbers to categories. We will soon see what this looks like when you have more than 2 groups. 
- Changing the numbers changes...?


## Effect coding
```{r}
example.data$tx.effect <- dplyr::recode(example.data$tx, '0' = -1, '1' = 1) 
```

```{r}
example.data
```


## effect coding

```{r}
mod.1.eff <- lm(traffic.risk ~ tx.effect, data = example.data)
tidy(mod.1.eff)
```
- systematically changes both the intercept and the regression estimate


## Interpretations

```{r, echo = FALSE}
effect <- tidy(mod.1.eff)
effect
dummy <- tidy(mod.1)
dummy
```
- Intercept: value when your predictor X is zero

- Regression coefficent: one unit increase in X is associated with a (regression estimate) increase in Y

## Effect coding
Consists of -1, 1s (And zeros for more than 2 groups)

1. The intercept is the "grand mean" or "mean of means" if unbalanced
2. The regression coefficient represents the group "effect" ie the difference between the grand mean and the group labeled 1 (we will revisit this when we have more than 2 groups as it will make more sense)

- Common to use for Factorial ANOVA models 

## Effect coding
Consists of -1, 1s (And zeros for more than 2 groups)

1. The intercept is the "grand mean" or "mean of means" if unbalanced
2. The regression coefficient represents the group "effect" ie the difference between the grand mean and the group labeled 1 (we will revisit this when we have more than 2 groups as it will make more sense)

- Common to use for Factorial ANOVA models 

- How do you calculate the predicted value of each group? 

## Dummy coding
- More appropriate when you are interested in comparing to a specific group rather than an "average person"  

- Intercept: value of the group coded zero 
- Regression coefficient: mean difference between groups 

## Contrast coding

- As our models get more complex our coding schemes can too
- What happens if you code the groups -.5 and .5? 



## Intermission

![](intermission.gif)


## Statistical Inference
- The way the world is = our model + error
- How good is our model? Does it "fit" the data well? 
- Need to go beyond asking if it is significant, because what does that mean? 
- We are going to make predictions and see if the predictions (based on our model) matches our data

## Predictions
- predictions $\hat{Y}$ are of the form of E(Y|X)
- They are created by simpling plugging a persons Xs into the created model
- If you have bs and have Xs you can create a prediction

$\hat{Y}_{i}$ = 2.6506410 + -0.4811057*$X_{i}$

- You have already done this with dummy codes above

## Predictions
- We want our predictions to be close to our actual data for each person ($Y_{i}$)
- The difference between the actual data and our our prediction ($Y_{i}  - \hat{Y}_{i} = e$) is the residual, how far we are "off". This tells us how good our fit is. 
- You can have the same estimates for two models but completely different fit. 
- Previously you have evaluated fit by looking at Eta Squared, SS Error and visualizing observations around group means



## Which one has better fit? 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
twogroup_fun = function(nrep = 100, b0 = 6, b1 = -2, sigma = 1) {
     ngroup = 2
     group = rep( c("group1", "group2"), each = nrep)
     eps = rnorm(ngroup*nrep, 0, sigma)
     traffic = b0 + b1*(group == "group2") + eps
     growthfit = lm(traffic ~ group)
     growthfit
}


twogroup_fun2 = function(nrep = 100, b0 = 6, b1 = -2, sigma = 2) {
     ngroup = 2
     group = rep( c("group1", "group2"), each = nrep)
     eps = rnorm(ngroup*nrep, 0, sigma)
     traffic = b0 + b1*(group == "group2") + eps
     growthfit = lm(traffic ~ group)
     growthfit
}

set.seed(16)
library(broom)
lm1 <- augment(twogroup_fun())

set.seed(16)
lm2 <- augment(twogroup_fun2())

plot1<- ggplot(lm1) +
  aes(x = group, y = traffic) +
  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)

plot2<- ggplot(lm2) +
  aes(x = group, y = traffic) +
  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)


library(gridExtra)
 grid.arrange(plot1, plot2, ncol=2)
```




## easy to examine fit with lm objects
- these are automatically created anytime you run a lm in R
```{r}
mod.1 <- lm(traffic.risk ~ tx, data = example.data)
```



```{r, eval=FALSE}
coefficients(mod.1)       # coefficients
residuals(mod.1)          # residuals
fitted.values(mod.1)      # fitted values ie predicted
summary(mod.1)$r.squared  # R-sq for the model
summary(mod.1)$sigma      # se of the model
```

##
```{r}
coefficients(mod.1)
```


## 
```{r}
fitted.values(mod.1)
```
## 

```{r}
residuals(mod.1)
```

## pop quiz, hotshot

$$ \hat{Y}_{i} = b_{0} + b_{1}X_{i}  $$

$$ Y_{i} = b_{0} + b_{1}X_{i} +e_{i}  $$
$$ Y_{i}  - \hat{Y}_{i} = e $$

- can you plug in numbers and calculate subject 3's predicted and residual scores without explicitly asking for lm object residuals and fitted values?

##
```{r}
example.data
```


## residuals
```{r, echo=FALSE, warnings=FALSE, message=FALSE, include = FALSE}
fit.1.data <- augment(mod.1) 

```

```{r}
ggplot(fit.1.data, aes(.resid)) +
    geom_density()    
   
```


## an aside concerning lm objects
lm objects consist of the information embeded in your linear model (ie mod.1). You use this information to get summary(mod.1) or anova(mod.1) as well as additional pieces of information. R handles model objects poorly because you cannot do advanced manipulation with any of this extra information due to them not being in a data frame.

```{r}
library(broom)
fit.1.tidy <- tidy(mod.1) #tidy turns the summary into a dataframe 
fit.1.tidy
```

##
- Augment ammends the original dataset with lm object content. The new variable names of have a "." infront of the name to distinguish

```{r, warning=FALSE}
fit.1.data <- augment(mod.1) 
head(fit.1.data)

```

## pop quiz, hotshot, part deux 
- how many different predicted values will we have? residuals? 


## Statistical Inference
- In making predictions, we have to compare our prediction to some alternative prediction to see if we are doing well or not. 

- What is our best guess (ie prediction) if we didn't collect any data? 

$$ \hat{Y} = \bar{Y} $$
- Regression can be thought of as: is E(Y|X) better than E(Y)?

## Statistical Inference
- To the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well

- Said differently, the closer our model is to the "actual" data generating model, our guesses (Y-hats) will be closer to our actual data (Ys)  

## Partitioning variation 

- We formally test how well we are doing with our guesses by partitioning variation

$$ Y = \hat{Y} + e$$

$$ Y = \hat{Y} + (Y - \hat{Y}) $$

$$ Y - \bar{Y} = (\hat{Y} -\bar{Y}) + (Y - \hat{Y}) $$

$$ (Y - \bar{Y})^2 = [(\hat{Y} -\bar{Y}) + (Y - \hat{Y})]^2 $$

$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$

## Partitioning the variation in Y
$$ \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2 $$

- SS total = SS between + SS within

- SS total = SS regression + SS residual (or error)

- Completely the same as last semester because REGRESSION IS ANOVA


## What can we do with this? 

- Last semester you did omnibus F tests
- What hypothesis does the omnibus F test test, generally? 

$$ s_{y}^2 = s_{regression}^2 + s_{residual}^2 $$

$$ 1 = \frac{s_{regression}^2}{s_{y}^2} + \frac{s_{residual}^2}{s_{y}^2}  $$


## Coefficient of Determination

$$ \frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2 $$

- percent (of total) variance explained by your model...which currently are groups
- another way of asking how much variance group status explains

## R squared and Eta squared

```{r}
summary(mod.1)$r.squared
```


```{r}
library(lsr)
etaSquared(mod.1)
```

## R squared for different coding schemes

```{r}
tidy(anova(mod.1))
```

```{r}
tidy(anova(mod.1.eff))
```

## Note that the R2 p-value
```{r,echo=FALSE}
summary(mod.1)
```


## summary

- We are using linear models to do the exact same tests as t tests and ANOVAs
- It is the exact same because t-tests and ANOVAs are part of the general linear model
- Looking at these tests through the perspective of a linear model provides us with all the information you got from last semester - and more! 
- Plus it provides a more systematic way at 1) building and testing your theoretical model and 2) comparing between alternative theoretical models
- You can get 1) estimates and 2) fit statistics from the model. Both are important. 
