---
title: "Factorial ANOVA"
output: ioslides_presentation
---  


## factorial ANOVA
- Between subjects (vs within) is where everyone is assigned to 1 group
- 2-way = 2 IVs, 3-way = 3 IVs, etc
- Advantages include: 1) efficiency 2) generalizability 3) interactions

## factorial ANOVA

|      |       |Factor| A          | Marginal Means       |
|------|-------|------|------------|----------------------|
|      |       | Tx   |  Control   |                      |
| B    |neutral|  7.6 |     5.0    |  6.33                |
|      | sad   |  8.3 |     5.0    |  6.66                |
|      | happy |  7.3 |     8.0    |  7.66                |
|______|_______|______|____________|______________________|
| M.M  |       | 7.77 |   6.0      | 6.88                 | 

## SS decomposition
- SStotal = SSbetween + SSwithin  
- SSbetween into three (or more) components (A,B,AxB)  


## Marginal vs cell means
|      |       |Factor| A          | Marginal Means       |
|------|-------|------|------------|----------------------|
|      |       | Tx   |  Control   |                      |
| B    |neutral|  7.6 |     5.0    |  6.33                |
|      | sad   |  8.3 |     5.0    |  6.66                |
|      | happy |  7.3 |     8.0    |  7.66                |
|______|_______|______|____________|______________________|
| M.M  |       | 7.77 |   6.0      | 6.88                 | 
- marginal for main effects  
- cell means for interaction  

## equation

- similar to effect coding within regression

$$ Y_{ijk} = \mu + \alpha_{j} + \beta_{k} + \alpha \beta_{jk}+\varepsilon_{ijk} $$

$$ \alpha_{j} =  \mu_{j} -  \mu $$

## interactions 
- main effects are on the marginal means  
- interactions are about the cell means  
- what is not expected based on the main effects (marginal means)
- similar to contingency table tests of independence from last semester

## Interpretting interactions part 1
- An interaction is present when the simple effects of one independent variable are not the same at all levels of the second independent variable. 
- If there in an interaction, main effects are hard to interpret, much more so than with continuous interactions 

## what do interactions with nominal data look like? 
- and can I "see" main effects too? 


## how is this similar to a linear model?
- is this: 
$$ Y_{ijk} = \mu + \alpha_{j} + \beta_{k} + \alpha \beta_{jk}+\varepsilon_{ijk} $$

- this?
$$ Y_{ijk} = b_{0} + b_{1}J + b_{2}K + b_{3}JK + \varepsilon_{ijk} $$

## Effect coding a factorial ANOVA
- j-1 and k-1 variables for main effects  
- (j-1)(k-1) variables for interaction  
<br>
$J_{ij}$ = 1 if in column j, -1 if in last column, 0 for all other columns  
$K_{ik}$ = 1 if in row k, -1 if in last row, 0 for all other rows  
$JK_{i}$ = pairwise product of each J with K variable  


## linear model equation

$$ Y_{ijk} = b_{0} + b_{1}J1 + b_{2}K1 + b_{3}K2 + b_{4}J1K1 + b_{5}J1K2 +\varepsilon_{ijk} $$


|      |       |Factor| A          | Marginal Means       |
|------|-------|------|------------|----------------------|
|      |       | Tx   |  Control   |                      |
| B    |neutral|  7.6 |     5.0    |  6.33                |
|      | sad   |  8.3 |     5.0    |  6.66                |
|      | happy |  7.3 |     8.0    |  7.66                |
|______|_______|______|____________|______________________|
| M.M  |       | 7.77 |   6.0      | 6.88                 | 


## coding

| row  |column |  J1  | K1  |  K2 | J1K1 | J1K2  |
|------|-------|------|-----|-----|------|-------|
| 1    |  1    |  1   |  1  |  0  |  1   |   0   |
| 1    |  2    | -1   |  1  |  0  | -1   |   0   |
| 2    |  1    |  1   |  0  |  1  |  0   |   1   |
| 2    |  2    | -1   |  0  |  1  |  0   |  -1   |
| 3    |  1    |  1   | -1  | -1  | -1   |  -1   |
| 3    |  2    | -1   | -1  | -1  |  1   |   1   | 


## but where is this in our output if ANOVA is regression? 


## example

```{r, echo=FALSE, message=FALSE}
data <- read.csv("~/Box Sync/5067 Regression Spring/5067 spring 18/Anxiety.data.csv")

library(dplyr)
# getting the group means 
Group_means<- data %>% group_by(group) %>% 
  summarise(meanAnxiety = mean(Anxiety), varStress = sd(Stress)^2,
            meanStress = mean(Stress), meanSupport=mean(Support))
# Calculating the grand mean by averaging the two group means together
Anxiety_grandmean <- mean(Group_means$meanAnxiety)
data$SupportLevel<-cut(data$Support, breaks = c(0,5,10,18), labels = c("low","med","high"))

data <- data %>% 
  dplyr::select(-Support) 
```

```{r}
data
```

##
```{r}
data %>% 
  select(group, SupportLevel) %>% 
  table()
```

## aov function
```{r}
aov.1 <- aov(Anxiety ~ group + SupportLevel + group*SupportLevel, data=data)
summary(aov.1)
```


## lm anova

```{r}
lm.1 <- lm(Anxiety ~ group + SupportLevel + group*SupportLevel, data=data)
anova(lm.1)
```

## lm summary with nominal variables
```{r, echo=FALSE}
library(broom)
tidy(lm.1)
```

$$ Y_{ijk} = b_{0} + b_{1}J1 + b_{2}K1 + b_{3}K2 + b_{4}J1K1 + b_{5}J1K2 +\varepsilon_{ijk} $$

## But what contrasts did R use? 

```{r}
contrasts(data$group)
contrasts(data$SupportLevel)
```
- we will see why this doesn't matter soon

## Interpretting interactions 
- beyond noticing whether cell means look different we can do formal tests  
- ~3 types  

## Interpretting interactions part 2
'1. Is the interaction significant? If not, this helps interpret our main effects. If it is, we can graph and interpret or also do additional tets.  

```{r}
aov.1 <- aov(Anxiety ~ group + SupportLevel + group*SupportLevel, data=data)
aov.0 <- aov(Anxiety ~ group + SupportLevel, data=data)

anova(aov.0, aov.1)
```

## Interpretting interactions part 2

'2. regression coefficients

$$ Y_{ijk} = b_{0} + b_{1}J1 + b_{2}K1 + b_{3}K2 + b_{4}J1K1 + b_{5}J1K2 +\varepsilon_{ijk} $$
- remember, this is likely done with default dummy

## Interpretting interactions part 2

'3. Post hoc comparisons/simple main effects

```{r}
TukeyHSD(aov.1)
```

## post hoc tests

- Planned contrasts, a priori: Holm, Dunn-Bonnferoni, perhaps Tukey
- All pairwise comparisons (maybe a priori): Tukey
- Post hoc fishing: Scheffe, perhaps Tukey if you can sleep at night

## estimated means
```{r, message = FALSE}
library(lsmeans)
lsm.1<- lsmeans(aov.1, c("group", "SupportLevel"))
# can be used to test numerous a prior and post hoc hypotheses
# especially important for graphing
```
##
```{r}
lsm.1
```



## also marginal means

```{r}
lsmeans(aov.1, specs = "group")
```

## lsmeans can also do tukey (and other adjustments)

```{r}
pairs(lsm.1, adjust = "tukey")
# try "holm" and "bon"
```

## post hoc tests

```{r}
# equivalent to pairs function above
#contrast(lsm.1, method = "pairwise")
contrast(lsm.1, method = "eff")
```


## post hoc tests

```{r, eval=FALSE}
library(multcomp)
```


## different types of Sums of Squares
- 3 types, 3 different hypotheses testing strategies   
- Type 1, hierarchical (what R does)  
- Type 3, simultaneous   
- Type 2, looks at higher- versus lower-order  
<br>  
- SSwithin and SSinteraction will be same, regardless  


## Type 1: Hierarchical (what R does) 
- Think of it as model comparisons  
A: anova(null, Factor A)  
B: anova(Factor A, FactorA+B)  
AB: anova(FactorA+B, FactorA+B+A*B)  
- SS is just like our partial correlation calculation for R2 when adding terms to hierarchical regression  

## Type 3: Controls for all predictors

A: anova(Factor B+A*B, FactorA+B+A*B)  
B: anova(Factor A+A*B, FactorA+B+A*B)  
AB: anova(FactorA+B, FactorA+B+A*B)    

- Most common, most recommended  
- Still strange, in that it tests higher order term without a lower order term in the model  
- Use Anova function from car package   



## Type 2: Not popular
A: anova(Factor B, FactorA+B)  
B: anova(Factor A, FactorA+B)  
AB: anova(FactorA+B, FactorA+B+A*B)  

- Need to use car::Anova function  
- Most interpretable, in my opinion.   



## Effect sizes
- Eta squared is like R squared for an effect SS effect / SS total    
- Partial eta squared (SS effect / (SS effect + SS resid))  
- the variance explained by a given variable of the variance remaining explained by other predictors  
- Partial will always be larger, but not interpretable with sig interaction   


##

```{r}
library(lsr)
etaSquared(aov.1)
```

## Don't forget omega squared and cohenâ€™s f

- Better than eta, adjusts for bias
- Omega Squared is always smaller than Eta Squared.  
- see: http://daniellakens.blogspot.com/2015/06/why-you-should-use-omega-squared.html   
- cohen's f is used in pwr  

## ANCOVA
- ANalysis of COVAriance
- aka regression with continuous and nominal variables
- given a special name because of historical reasons
- Typically, when someone says they are doing an ANCOVA they are focused on comparing group means adjusted for some covariate

## ANCOVA

$$ \hat{Y} = b_{0} + b_{1}D_{1} + b_{2}D_{2} + b_{3}GPA  $$
- how do you interpret each term?
- what does this look like graphed? 
- Would b1 and b2 change if we ran a model without GPA? 
 

## 2 advantages of ANCOVA
1. Greater power
2. Adjustment of/controlling for covariate

## ANCOVA power advantages
- what is SS residual? 
- how is it used to test your model? 
- how can you decrease SS residual? 

## ANCOVA power advantages
- power does not necessarily incrase if covariate is related to the predictor (e.g., group). Why?
- how do you find groups not related to the covariate?

## Controlling for background covariates
- in experimental designs, power should be main focus
- in quasi-experimental designs, covariates should help "equate" groups

$$ \hat{Y} = b_{0} + b_{1}D_{1} + b_{2}D_{2} + b_{3}GPA  $$

- note: you are now making a different prediction for each individual, rather than having the same prediction for all group members 
- likely also reduces SS residual

## Controlling for background covariates
- controlling for is not a panacea
- does not fix issues when random assigment is ineffective
- could create greater group difference, no difference or even reverse the difference


## Standard ANCOVA assumes no interactions; regression association is the same across groups

$$ \hat{Y} = b_{0} + b_{1}D_{1} + b_{2}D_{2} + b_{3}GPA  $$

$$ \hat{Y} = b_{0} + b_{1}D_{1} + b_{2}D_{2} + b_{3}GPA + b_{4}(D_{1} * GPA) + b_{5}(D_{2} * GPA)  $$

